{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Duffy BOW and LR unprocessed data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "30mk5SLRCY80",
        "outputId": "5b453a2e-c666-446d-8bac-a871889ae47e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a27750f1-2950-42b8-86b6-707594afe00e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a27750f1-2950-42b8-86b6-707594afe00e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_unstacked.csv to data_unstacked (3).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io"
      ],
      "metadata": {
        "id": "yqZ63qDCDgf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded['data_unstacked.csv']))"
      ],
      "metadata": {
        "id": "tz8hGsTBEb5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collapsing the outcome variable into two values: 1 denoting the line relates to symptoms and zero otherwise\n",
        "df[\"SYMPTOMS\"] = df[\"SYMPTOMS\"] > 1.5"
      ],
      "metadata": {
        "id": "txgXBkntEkXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bag of words - taken from 6.871 PS1\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer "
      ],
      "metadata": {
        "id": "Kt_iENJRJvCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: this function was inspired by the following link's preprocessing function: \n",
        "# https://kavita-ganesan.com/how-to-use-countvectorizer/#.Yg0SrujMJdh\n",
        "def my_cool_preprocessor(text):\n",
        "    text=text.lower() # convert to all lower case\n",
        "    text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
        "    text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
        "    text = re.sub(r'\\d+', '', text) # remove digits\n",
        "    return text\n",
        "\n",
        "corpus = df[\"TURN_TEXT\"]\n",
        "count_vec = CountVectorizer(max_features = 100, stop_words='english', preprocessor = my_cool_preprocessor)\n",
        "count_data = count_vec.fit_transform(corpus)\n",
        "cv_data = pd.DataFrame(count_data.toarray(), columns = count_vec.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "eEv9yF3YLfXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined = pd.concat([df, cv_data], axis=1)"
      ],
      "metadata": {
        "id": "6psvpbhfLh_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create subdatasets for train/val/test\n",
        "df_train, df_valid, df_test = np.split(df_combined.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(df_combined)), int(.8*len(df_combined))])\n",
        "\n"
      ],
      "metadata": {
        "id": "aZ-RgfpKLmPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the logistic regression using the text features as features\n",
        "feat_names_lab = list(df_combined)[20:]\n",
        "X_df_train = df_train[feat_names_lab].values\n",
        "y_df_train = df_train['SYMPTOMS'].values\n",
        "\n",
        "X_df_valid = df_valid[feat_names_lab].values\n",
        "y_df_valid = df_valid['SYMPTOMS'].values\n",
        "\n",
        "X_df_test = df_test[feat_names_lab].values\n",
        "y_df_test = df_test['SYMPTOMS'].values"
      ],
      "metadata": {
        "id": "P_leM8nzMtD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a loop going through all C and penalty values, select the values with best validation accuracy (see chunk below)\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "d = {'C': [0.1,0.25,0.5,1., 0.1,0.25,0.5,1.], 'penalty': ['l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2']}\n",
        "d = pd.DataFrame(data=d)\n",
        "for i in d.index:\n",
        "  pipe = Pipeline([('norm', StandardScaler()), ('LR', LogisticRegression(C=d[\"C\"][i], penalty=d[\"penalty\"][i], solver='liblinear'))])\n",
        "  pipe.fit(X_df_train, y_df_train)\n",
        "  ypred = pipe.predict(X_df_valid)\n",
        "  #accuracy\n",
        "  d.loc[i, \"validation_acc\"] = accuracy_score(y_df_valid, ypred)\n",
        "\n",
        "max_validation_acc = d[\"validation_acc\"].max()\n",
        "C_best = d[d[\"validation_acc\"] == max_validation_acc ][[\"C\"]].squeeze()\n",
        "penalty_best = d[d[\"validation_acc\"] == max_validation_acc ][[\"penalty\"]].squeeze()\n",
        "\n",
        "#on test data\n",
        "ypred2 = pipe.predict(X_df_test)"
      ],
      "metadata": {
        "id": "jjvNak1gMvRw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4ba6dcc0-3553-41da-f8d5-dabe824dc354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a979d904a222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# getting predictions on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'LR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mmodel_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mypred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mSAGA\u001b[0m \u001b[0msolver\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mfloat64\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfloat32\u001b[0m \u001b[0mbit\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \"\"\"\n\u001b[0;32m-> 1461\u001b[0;31m         \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"elasticnet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_penalties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         raise ValueError(\n\u001b[1;32m    442\u001b[0m             \u001b[0;34m\"Logistic Regression supports only penalties in %s, got %s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1538\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LRoN8GnFmDcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "output = {'feature': feat_names_lab, 'coefficient': model_best.named_steps['LR'].coef_[0] }\n",
        "output = pd.DataFrame(data=output)\n",
        "output['abs_coeff'] = output['coefficient'].abs()\n",
        "features_ranked = output.sort_values(by=['abs_coeff'], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "epND_SszhggY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bootstrapping accuracy and AUC\n",
        "%pip install scipy==1.7.1\n",
        "from scipy.stats import bootstrap\n",
        "thresh = 0.5\n",
        "boot_auc = bootstrap((y_df_test,ypred2 > thresh), roc_auc_score, vectorized = False, paired = True, random_state = 42, method = 'basic', n_resamples = 1000)\n",
        "(boot_auc.confidence_interval.high + boot_auc.confidence_interval.low)/2\n"
      ],
      "metadata": {
        "id": "DzEeMuWEz66_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb44358-8d72-43e1-e360-61ce17568fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy==1.7.1 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.7.1) (1.21.6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.578444062939651"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boot_auc.standard_error"
      ],
      "metadata": {
        "id": "KD_qNYeD0FwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e0d813-6295-485f-b0d0-66d01548ad6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0016304205465390406"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boot_acc = bootstrap((y_df_test,ypred2 > thresh), accuracy_score, vectorized = False, paired = True, random_state = 42, method = 'basic', n_resamples = 1000)\n",
        "(boot_acc.confidence_interval.high + boot_acc.confidence_interval.low)/2"
      ],
      "metadata": {
        "id": "XK197Yky0IUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7a54dd-374b-466f-d43d-d004d127fe8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8835925611929916"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boot_acc.standard_error"
      ],
      "metadata": {
        "id": "0SKtsFIL0MzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3436f6-7ad3-4390-cdf7-e9389969b0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0021410830358185146"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiana's Part "
      ],
      "metadata": {
        "id": "jiCmQ22RxzOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the LR Model"
      ],
      "metadata": {
        "id": "3bPxeGAn1qKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract model from pipeline\n",
        "lr = pipe['LR']"
      ],
      "metadata": {
        "id": "5VfMksKAvmaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I evaluate the model from prediction imbalance, AUC, and sensitivity\n",
        "\n",
        "1. The LR model only predicts 0 or 3. Possibly due to the fact that class 1 and class 2 consists of only a tiny proportion of the data to begin with. \n",
        "\n",
        "2. Weighted multiclass AUC is 0.693. Out of the 4 classes, class 0 and 3 have individual AUC (OVR) of 0.73 while the other two classes only have AUC 0.63 and 0.68.\n",
        "\n",
        "3. Sensitivity for class 3 is at 2%. Out of 2460 cases with class 3, the model identified 54 cases correctly."
      ],
      "metadata": {
        "id": "Vog6grZ7x7zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of times each class appeared in predictions\n",
        "from collections import Counter\n",
        "Counter(lr.predict(X_df_test))"
      ],
      "metadata": {
        "id": "Nb6CttTMvnda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of times each class appeared in actual dataset \n",
        "Counter(y_df_test)"
      ],
      "metadata": {
        "id": "aVS1jRkMweNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted average of auc across four classes\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_df_test, lr.predict_proba(X_df_test), multi_class='ovr')"
      ],
      "metadata": {
        "id": "GlfJcFffyYL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get one-versus-rest auc score for each class\n",
        "for label in [0,1,2,3]:\n",
        "  y_bin = [1 if x == label else 0 for x in y_df_test]\n",
        "  pred = lr.predict_proba(X_df_test)[:,label]\n",
        "  auc = roc_auc_score(y_bin, pred)\n",
        "  print(f\"Class {label} has OVR AUC of {auc}\")"
      ],
      "metadata": {
        "id": "FeCS8ph0zg4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sensitivity for class 3 \n",
        "pred3 = np.array([1 if x == 3 else 0 for x in lr.predict(X_df_test)])\n",
        "true3 = np.array([1 if x == 3 else 0 for x in y_df_test])\n",
        "\n",
        "sum(pred3[true3 == 1])/sum(true3[true3 == 1])"
      ],
      "metadata": {
        "id": "99HK-G2Z5LIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance of the LR Model"
      ],
      "metadata": {
        "id": "Rew-s3OK2eSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike we conjectured, most of the words that have the highest influence on the probability of being class 3 are **NOT** directly describing the symptoms. Instead they tend to be more related to how patients express their feelings, i.e. \"like\", \"feeling\", \"feel\", \"bad\".\n",
        "\n",
        "Also, the nature of bag-of-words made it hard to recover the contextual relationship between words used in a sentence. For instance, the model is sensitive to the word \"feel\", but it does not capture what the patient said after the word \"feel\", i.e. the actual symptom that we want to extract. "
      ],
      "metadata": {
        "id": "GFbocHJU2xeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({\n",
        "    'features': feat_names_lab,\n",
        "    'coefs': lr.coef_[-1,:]\n",
        "})"
      ],
      "metadata": {
        "id": "dOFSFYDJv7ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance.sort_values(by = 'coefs', ascending = False).head(20)"
      ],
      "metadata": {
        "id": "jAIa0LNFxXYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying XGBoost "
      ],
      "metadata": {
        "id": "xnQw_WOO756A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "import xgboost as xgb\n",
        "\n",
        "#do gridsearch to find best parameters for XGBoost \n",
        "def run_xgb(x_train, y_train, metric = 'roc_auc_ovr_weighted', max_depth = np.arange(2,6,2), learning_rate = [0.1], n_estimators = np.arange(100,350,50)):\n",
        "    print('running gridsearch for xgb...')\n",
        "    classes_weights = class_weight.compute_sample_weight(\n",
        "                                class_weight='balanced',\n",
        "                                y=y_train\n",
        "                        )\n",
        "    cv_folds = 3\n",
        "    param_grid = {'max_depth': max_depth,\n",
        "                  'learning_rate':learning_rate,\n",
        "                  'n_estimators':n_estimators\n",
        "                 }\n",
        "    \n",
        "    est = xgb.XGBClassifier(verbosity=1, objective = \"multi:softmax\", num_class = 4, sample_weight = classes_weights)\n",
        "\n",
        "    #grid search\n",
        "    gs = GridSearchCV(estimator = est, \n",
        "                            param_grid=param_grid, \n",
        "                            scoring = metric, \n",
        "                            cv= cv_folds, \n",
        "                            verbose=1)\n",
        "    gs.fit(x_train, y_train, sample_weight = classes_weights)\n",
        "    print(gs.best_params_)\n",
        "\n",
        "    return gs.best_estimator_"
      ],
      "metadata": {
        "id": "hFq6-4jq7cS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = run_xgb(X_df_train, y_df_train)"
      ],
      "metadata": {
        "id": "QmfRRvy27Ym0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of XGBoost (Using same metric as before)\n",
        "\n",
        "We observe that the AUC remains pretty much the same, but prediction imbalance and sensitivity improved a lot compared to LR."
      ],
      "metadata": {
        "id": "PJnsvQdIHPKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "#check model prediction breakdown \n",
        "Counter(model.predict(X_df_test))"
      ],
      "metadata": {
        "id": "zXkaoxXEHWM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = Counter(y_df_test)"
      ],
      "metadata": {
        "id": "eL7jnJ1NTOVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in d.items():\n",
        "  print((key, value/len(y_df_test)))"
      ],
      "metadata": {
        "id": "QHh3SkdtTmuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "lJ2tV1McNo7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted average of auc across four classes\n",
        "roc_auc_score(y_df_test, model.predict_proba(X_df_test), multi_class='ovr')"
      ],
      "metadata": {
        "id": "hMVin4vTGHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get one-versus-rest auc score for each class\n",
        "for label in [0,1,2,3]:\n",
        "  y_bin = [1 if x == label else 0 for x in y_df_test]\n",
        "  pred = model.predict_proba(X_df_test)[:,label]\n",
        "  auc = roc_auc_score(y_bin, pred)\n",
        "  print(f\"Class {label} has OVR AUC of {auc}\")"
      ],
      "metadata": {
        "id": "hhGSAmPkGdqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sensitivity for class 3 \n",
        "pred3 = np.array([1 if x == 3 else 0 for x in model.predict(X_df_test)])\n",
        "true3 = np.array([1 if x == 3 else 0 for x in y_df_test])\n",
        "\n",
        "sum(pred3[true3 == 1])/sum(true3[true3 == 1])"
      ],
      "metadata": {
        "id": "noIWDLZGGjzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#precision \n",
        "precision_score(true3, pred3)"
      ],
      "metadata": {
        "id": "MWtajUsHOo18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import pickle\n",
        "# pickle.dump(model, open('/content/drive/My Drive/xgb_6871_project.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "_Pu02F4WLftn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model = pickle.load(open('/content/drive/My Drive/xgb_6871_project.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "jUWsG6vhKiUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shap Values\n",
        "\n",
        "For XGBoost, analyzing feature importance is a bit tricky because the default feature importance attribute does not tell whether a word increases or decreases the probability of a certain prediction. Instead we can use SHAP plots."
      ],
      "metadata": {
        "id": "pMuzKrWDIDu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap \n",
        "explainer = shap.Explainer(model)"
      ],
      "metadata": {
        "id": "SAKAetlGG4Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_obj = explainer(X_df_train)"
      ],
      "metadata": {
        "id": "qMOUbXVcJN27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot shap values for class 3\n",
        "shap.summary_plot(shap_obj[:,:,-1], max_display=15, feature_names = feat_names_lab)"
      ],
      "metadata": {
        "id": "7dLOSdbKJgmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to interpret SHAP plots: \n",
        "1. The y-axis indicates the variable name, in order of importance from top to \n",
        "bottom. The value next to them is the mean SHAP value.\n",
        "2. On the x-axis is the SHAP value. Indicates how much is the change in log-odds per increment in the frequency of a particular word.\n",
        "\n",
        "See this [link](https://blog.datascienceheroes.com/how-to-interpret-shap-values-in-r/) for more info. \n",
        "\n",
        "Like Logistic Regression, looking at feature importance by XGBoost is NOT helpful for symptom tagging. The only word directly describing symtom is \"pain\". \n",
        "\n",
        "I wonder if keeping connector words like \"and\"/\"or\" is necessary here as it seems to add noise to the datset (both models considered the connector words as important feature). \n"
      ],
      "metadata": {
        "id": "WB3IxUspPnxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Step \n",
        "\n",
        "I think before we go on tuning the model, we should spend more time improving the vectorization. Some possible directions are\n",
        "1. Removing words that are highly frequent throughout the corpus, i.e. the connector words and filler words. Methods include adjusting max_df/min_df and/or using TF-IDF.\n",
        "2. Using not just unigram but also bigrams/trigrams\n"
      ],
      "metadata": {
        "id": "SbLQZtcESM69"
      }
    }
  ]
}